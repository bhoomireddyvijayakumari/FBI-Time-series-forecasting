{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "iky9q4vBYrdO",
        "y-Ehk30pYrdP",
        "578E2V7j08f6",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhoomireddyvijayakumari/FBI-Time-series-forecasting/blob/main/paisa_bazaar_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Paisa Bazaar Project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Paisa Bazaar Project is a comprehensive data science initiative focused on analyzing customer credit behavior to predict credit scores using exploratory data analysis (EDA) and machine learning techniques. The dataset encompasses a wide range of financial and demographic variables, including annual income, occupation, number of loans, delayed payments, credit utilization ratios, and more. Through meticulous data preprocessing—such as handling missing values, encoding categorical features, and creating new financial ratios—the project ensures high-quality input for modeling. Visualization techniques, including histograms, box plots, and bar charts, reveal critical insights, such as the dominance of developers in loan uptake and the correlation between delayed payments and specific occupations. Two machine learning models, Random Forest and XGBoost, were implemented and evaluated, with Random Forest achieving superior performance (83.6% accuracy) in classifying customers into credit score categories (Good, Poor, Standard). The project not only identifies key predictors of creditworthiness but also provides a scalable framework for financial institutions to assess risk and tailor products effectively.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project addresses the challenge of accurately predicting customer credit scores to optimize financial product offerings and risk management. Financial institutions often struggle with assessing creditworthiness due to the complexity of customer behaviors and diverse financial backgrounds. By leveraging machine learning, this project aims to provide a data-driven solution to classify customers into credit score categories (Good, Poor, Standard) based on their financial history and demographics. The goal is to enable banks and lenders to make informed decisions, reduce default risks, and tailor financial products to individual customer profiles."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/PBP/dataset-2.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "pd.set_option('display.max_columns',None)\n",
        "# This line will allow all the columns to display in the dataframe\n",
        "# Dataset First Look\n",
        "df.head()\n",
        "#This will show the first 5 rows of the dataset"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n",
        "# Here shape method will return rows, columns as output"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n",
        "# This will return all of the columns with non-null count and also the datatypes of the columns."
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "#There are no missing values as seen from previous results."
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis reveals a working-age dominant population (20-50 years), with peak representation at 40 years. Income distribution is right-skewed (median: ₹50,000), indicating a majority in moderate income brackets alongside a limited high-earner segment. Credit behavior shows clear stratification: good credit scores correlate with financial stability (fewer delays, higher income), while poor scores associate with payment delinquency and income volatility. Occupation data highlights a strong tech-sector presence, with distinct financial patterns across roles. Seasonal trends in credit activity suggest cyclical financial behaviors. These findings support segmented financial strategies for optimized risk management and targeted product offerings.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "dPxRM3lqFKKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n",
        "#here it shows the datatype of each column"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset includes ID (unique record identifier), Customer_ID (unique customer identifier), and Month (time period of data recording), along with demographic variables like Name, Age, SSN (sensitive identifier), and Occupation, which help segment customers by life stage and profession. Financial attributes include Annual_Income, Monthly_Inhand_Salary (disposable income), Num_Bank_Accounts, and Num_Credit_Card, reflecting financial activity, while loan-related variables such as Interest_Rate, Num_of_Loan, Type_of_Loan, Total_EMI_per_month, and Outstanding_Debt assess debt exposure. Credit behavior is captured through Delay_from_due_date, Num_of_Delayed_Payment (risk indicators), Credit_Utilization_Ratio, Credit_Mix, Num_Credit_Inquiries, and Credit_History_Age, which influence credit scoring. Additional variables like Payment_of_Min_Amount (payment discipline), Amount_invested_monthly (savings behavior), and Monthly_Balance (liquidity) provide insights into financial health, culminating in the Credit_Score, a categorical measure of overall creditworthiness. Together, these variables enable segmentation, risk assessment, and tailored financial strategies.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "Payment_of_Min_Amount=df.loc[:,'Payment_of_Min_Amount'].unique()\n",
        "Occupation=df.loc[:,'Occupation'].unique()\n",
        "credit_mix=df.loc[:,'Credit_Mix'].unique()\n",
        "Credit_Score=df.loc[:,'Credit_Score'].unique()\n",
        "Payment_Behaviour=df.loc[:,'Payment_Behaviour'].unique()\n",
        "print(f'Occupation:{Occupation}')\n",
        "print(f'Payment_of_Min_Amount: {Payment_of_Min_Amount}')\n",
        "print(f'Credit_Score:{Credit_Score}')\n",
        "print(f'credit_mix: {credit_mix}')\n",
        "print(f'Payment_Behaviour: {Payment_Behaviour}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing all the rows, where Payment_of_Min_Amount is NM\n",
        "df = df[df['Payment_of_Min_Amount'] != 'NM']"
      ],
      "metadata": {
        "id": "edWupo6qJJIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing all the rows, where Num_of_loan is 0\n",
        "df= df[df['Num_of_Loan'] != 0]\n",
        "df"
      ],
      "metadata": {
        "id": "gkvMA3gHJkOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Payment Behavior: Removing rows with unspecified minimum payment information ensures that only customers with clear payment records are analyzed, leading to better insights into the relationship between payment habits and credit scores.\n",
        "\n",
        "Removing Rows with Num_of_Loan = 0:\n",
        "Rows where Num_of_Loan is 0 were removed to focus the analysis on customers who have active loans. By removing rows with Num_of_Loan as 0, the analysis zeroes in on customers actively using credit, providing more relevant insights into credit utilization and repayment behaviors.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code - Annual Income Distribution\n",
        "plt.figure(figsize=(4,4))\n",
        "sns.histplot(x=df['Annual_Income'],kde=True,bins=20, color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram with a density curve is ideal for this analysis because:\n",
        "Displays Income Spread Across a Population. It helps visualize how annual income is distributed, making it clear which income ranges are most common.\n",
        "Identifies Skewness in Income Data. It shows whether income levels are concentrated in lower or higher ranges."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Customers Earn Between 0 and 50,000 Annually.Gradual Decline in Count Beyond 50,000."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "Targeted Pricing Strategies- Helps businesses optimize pricing models based on the most common income groups."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YbTIIBOkLiXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code- Delayed payments according to occupation\n",
        "plt.figure(figsize=(9,6))\n",
        "sns.boxplot(y=df['Num_of_Delayed_Payment'],hue=df['Occupation'],palette='magma')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It helps in identifying outliers and  compares occupational payment behavior."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some occupations exhibit greater variability in delayed payments.Occupations such as Doctors, Engineers, and Accountants show less variation, indicating financial reliability and timely repayments."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code -  Which occupation has highest number of customers in the given dataset\n",
        "# Drop duplicate rows for Name and keep unique combinations of Name and Occupation\n",
        "unique_occupation = df[['Name', 'Occupation']].drop_duplicates()\n",
        "unique_occupation['Occupation']\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.countplot(y=unique_occupation['Occupation'],color='green')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A count plot is ideal for this analysis because it effectively displays frequency distribution .\n",
        "\n",
        "Career Demand & Workforce Planning - Helps identify which professions are in high demand and which ones have fewer representatives, supporting industry growth strategies."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developer Has the Highest Representation. Scientist Has the Lowest Count.  Balanced Presence of Engineers, Managers, Accountants, and Lawyers.\n",
        "Creative Fields Have Moderate Presence."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Companies can refine recruitment efforts based on high-demand professions."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code -  credit inquiries fluctuate across different months\n",
        "monthly_credit_enquiries = df.groupby('Month')['Num_Credit_Inquiries'].sum()\n",
        "monthly_credit_enquiries\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "monthly_credit_enquiries.plot(kind='bar', color='pink', edgecolor='black')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Total Credit Enquiries\")\n",
        "plt.title(\"Total Credit Enquiries per Month\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--',alpha=0.8)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is chosen because: Categorical Nature of the Data, Clear Representation of Volume."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the months of June to August, there is a noticeable surge in credit enquiries, indicating heightened financial activity and increased demand for credit.\n",
        "\n",
        "Between January and April, there is a noticeable decline in credit enquiries, suggesting reduced financial activity and lower demand for credit during this period."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code-Average Number of Loans Taken Across Different Occupations\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.barplot(data=df,y='Occupation',x='Num_of_Loan',palette='magma')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart is ideal for this analysis because:\n",
        "Clear Comparison Across Occupations,\n",
        "Effective for Categorized Data."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developers Have the Highest Average Number of Loans. Scientists and Teachers Have the Lowest Loan Counts. Occupation Influences Loan Behavior.\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Banks can tailor loan offerings based on occupation-specific trends, ensuring targeted financial solution."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Does Occupation Affect the Number of Delayed Payments?\n",
        "2. Are Monthly Balances Different Between 'Good' and 'Poor' Credit Score Customers?\n",
        "3. Is the Average Annual Income Different for Customers with Different Credit Scores?"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NH:  Occupation Affects the Number of Delayed Payments.\n",
        "AH:  Occupation doesnt Affect the Number of Delayed Payments."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Grouping by occupation\n",
        "occupations = df['Occupation'].unique()\n",
        "groups = [df[df['Occupation'] == occ]['Num_of_Delayed_Payment'].dropna() for occ in occupations]\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = f_oneway(*groups)\n",
        "print(f\"ANOVA F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\" Reject H0: There is a significant difference in delayed payments across occupations.\")\n",
        "else:\n",
        "    print(\" Fail to reject H0: No significant difference in delayed payments across occupations.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way ANOVA"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparing means across multiple categories."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NH: Monthly Balances Are Different Between 'Good' and 'Poor' Credit Score Customers.\n",
        "AH:Monthly Balances Are Not Different Between 'Good' and 'Poor' Credit Score Customers.\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "good_bal = df[df['Credit_Score'] == 'Good']['Monthly_Balance'].dropna()\n",
        "poor_bal = df[df['Credit_Score'] == 'Poor']['Monthly_Balance'].dropna()\n",
        "\n",
        "t_stat, p_value = ttest_ind(good_bal, poor_bal, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\" Reject H0: Monthly balance differs significantly between Good and Poor credit scorers.\")\n",
        "else:\n",
        "    print(\" Fail to reject H0: No significant difference in monthly balance.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Independent t-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balances are different."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NH:The Average Annual Income is Different for Customers with Different Credit Scores.\n",
        "AH:The Average Annual Income is not Different for Customers with Different Credit Scores."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "groups = [df[df['Credit_Score'] == cs]['Annual_Income'].dropna() for cs in df['Credit_Score'].unique()]\n",
        "f_stat, p_value = f_oneway(*groups)\n",
        "print(f\"ANOVA F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: Income differs significantly by credit score category.\")\n",
        "else:\n",
        "    print(\"Fail to reject H0: No significant income difference across credit scores.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way ANOVA"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparing means across multiple categories."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.drop(columns=['Name', 'SSN'], inplace=True)  # Not useful for modeling\n",
        "\n",
        "# Handle missing values\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['float64', 'int64']:\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "    else:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values of variables with datatypes float and int have been replaced with median and rest with mean."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "for col in cat_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode the target label BEFORE train-test split\n",
        "le = LabelEncoder()\n",
        "df['Credit_Score'] = le.fit_transform(df['Credit_Score'])\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encoding."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "df['Debt_to_Income'] = df['Outstanding_Debt'] / (df['Annual_Income'] + 1)\n",
        "\n",
        "# Create EMI to monthly salary ratio\n",
        "df['EMI_to_Salary'] = df['Total_EMI_per_month'] / (df['Monthly_Inhand_Salary'] + 1)\n",
        "\n",
        "# Create new feature: Credit utilization efficiency\n",
        "df['Utilization_Efficiency'] = df['Credit_Utilization_Ratio'] / (df['Num_Credit_Card'] + 1)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now split\n",
        "X = df.drop(\"Credit_Score\", axis=1)\n",
        "y = df[\"Credit_Score\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20. because its the standard ratio and best for this small dataset."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "xmkqD9wqdWnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77f2e88-e74e-4a7f-ff0f-c9cdf31b73b5"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[2899   12  655]\n",
            " [  27 5019  753]\n",
            " [ 680 1148 8807]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.81      3566\n",
            "           1       0.81      0.87      0.84      5799\n",
            "           2       0.86      0.83      0.84     10635\n",
            "\n",
            "    accuracy                           0.84     20000\n",
            "   macro avg       0.83      0.84      0.83     20000\n",
            "weighted avg       0.84      0.84      0.84     20000\n",
            "\n",
            "\n",
            "Accuracy Score: 0.83625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=42)\n",
        "from scipy.stats import randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rf_params = {\n",
        "    'n_estimators': randint(50, 150),\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "# Fast randomized search\n",
        "rf_search = RandomizedSearchCV(\n",
        "    rf, rf_params, n_iter=5, cv=3, scoring='accuracy',\n",
        "    n_jobs=-1, verbose=1, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "rf_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "rf_best = rf_search.best_estimator_\n",
        "rf_pred = rf_best.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n Random Forest Results\")\n",
        "print(\"Best Params:\", rf_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPDYYJKooJ5w",
        "outputId": "5f3d5fb1-4974-4817-9b77-9f7ce89b3f28"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            " Random Forest Results\n",
            "Best Params: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 71}\n",
            "Accuracy: 0.8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "randomsearch CV- it uses random values to assess performance using the combinations provided."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nope."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Train the model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 2. Predict\n",
        "y_pred = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "# 3. Evaluate\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "print(\"\\n XGBoost Evaluation\")\n",
        "print(\"Accuracy Score:\", acc)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29ce7a0-dcf8-4039-dc87-caf454f32acf"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " XGBoost Evaluation\n",
            "Accuracy Score: 0.79515\n",
            "Confusion Matrix:\n",
            " [[2765   26  775]\n",
            " [ 157 4534 1108]\n",
            " [ 817 1214 8604]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76      3566\n",
            "           1       0.79      0.78      0.78      5799\n",
            "           2       0.82      0.81      0.81     10635\n",
            "\n",
            "    accuracy                           0.80     20000\n",
            "   macro avg       0.78      0.79      0.79     20000\n",
            "weighted avg       0.80      0.80      0.80     20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Base model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Small and fast param grid\n",
        "xgb_params = {\n",
        "    'n_estimators': randint(50, 150),\n",
        "    'max_depth': [3, 6],\n",
        "    'learning_rate': uniform(0.05, 0.15),\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "#randomized search\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    xgb, xgb_params, n_iter=5, cv=3, scoring='accuracy',\n",
        "    n_jobs=-1, verbose=1, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "xgb_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "xgb_best = xgb_search.best_estimator_\n",
        "xgb_pred = xgb_best.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\n XGBoost Results\")\n",
        "print(\"Best Params:\", xgb_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, xgb_pred))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc786763-f285-4a61-fda1-ec08965bdb69"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "⚡ XGBoost Results\n",
            "Best Params: {'colsample_bytree': 0.8, 'learning_rate': np.float64(0.17992642186624025), 'max_depth': 6, 'n_estimators': 73, 'subsample': 0.8}\n",
            "Accuracy: 0.75975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Random search cause it randomly checks best possible combinations of parameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nope."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the metrics from the Confusion matrix."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest was chosen because of better accuracy."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Save model\n",
        "joblib.dump(rf_best, \"best_random_forest_model.pkl\")\n",
        "# Save scaler too (for future prediction scaling)\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861e85e5-9ff4-4311-eb03-09e8d4039a2d"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample new input with ALL original training columns\n",
        "new_data = pd.DataFrame([{\n",
        "    'ID': 999999,\n",
        "    'Customer_ID': 888888,\n",
        "    'Month': 12,\n",
        "    'Age': 29,\n",
        "    'Occupation': 3,\n",
        "    'Annual_Income': 55000.0,\n",
        "    'Monthly_Inhand_Salary': 4580.0,\n",
        "    'Num_Bank_Accounts': 4,\n",
        "    'Num_Credit_Card': 3,\n",
        "    'Interest_Rate': 11,\n",
        "    'Num_of_Loan': 2,\n",
        "    'Type_of_Loan': 5,\n",
        "    'Delay_from_due_date': 6,\n",
        "    'Num_of_Delayed_Payment': 2,\n",
        "    'Changed_Credit_Limit': 800.0,\n",
        "    'Num_Credit_Inquiries': 1,\n",
        "    'Credit_Mix': 2,\n",
        "    'Outstanding_Debt': 2100.0,\n",
        "    'Credit_Utilization_Ratio': 31.5,\n",
        "    'Credit_History_Age': 102,\n",
        "    'Payment_of_Min_Amount': 1,\n",
        "    'Total_EMI_per_month': 600.0,\n",
        "    'Amount_invested_monthly': 400.0,\n",
        "    'Payment_Behaviour': 4,\n",
        "    'Monthly_Balance': 950.0,\n",
        "    'Credit_Score': 1,\n",
        "    'Debt_to_Income': 0.38,\n",
        "    'EMI_to_Salary': 0.13,\n",
        "    'Utilization_Efficiency': 0.72\n",
        "}])\n",
        "\n",
        "# Load saved model and scaler\n",
        "rf_model = joblib.load(\"best_random_forest_model.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "# Get feature names used during training\n",
        "# These must exactly match training order\n",
        "feature_names = [\n",
        "    'ID', 'Customer_ID', 'Month', 'Age', 'Occupation', 'Annual_Income',\n",
        "    'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
        "    'Interest_Rate', 'Num_of_Loan', 'Type_of_Loan', 'Delay_from_due_date',\n",
        "    'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries',\n",
        "    'Credit_Mix', 'Outstanding_Debt', 'Credit_Utilization_Ratio',\n",
        "    'Credit_History_Age', 'Payment_of_Min_Amount', 'Total_EMI_per_month',\n",
        "    'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance',\n",
        "    'Debt_to_Income', 'EMI_to_Salary', 'Utilization_Efficiency'\n",
        "]\n",
        "\n",
        "# Use only feature columns (drop target if present)\n",
        "X_input = new_data[feature_names]\n",
        "\n",
        "# Scale features\n",
        "X_scaled = scaler.transform(X_input)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_scaled)\n",
        "\n",
        "# Decode predicted label (if label encoded)\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = ['Good', 'Poor', 'Standard']  # adjust as needed\n",
        "\n",
        "\n",
        "\n",
        "print(\"🎯 Predicted Credit Score:\", y_pred[0])\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0dd9e3-261c-4e35-e93f-d03df3b31673"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Predicted Credit Score: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Paisa Bazaar Project demonstrates the power of data-driven approaches in transforming credit risk assessment. Through rigorous EDA, the project uncovers actionable insights, such as the influence of occupation on loan behavior and the seasonal trends in credit inquiries. The Random Forest model, with its 83.6% accuracy, outperforms XGBoost, highlighting its suitability for this classification task. Key takeaways include the importance of debt-to-income ratios, payment discipline, and credit utilization in determining credit scores. For future work, integrating real-time data streams and deploying the model as an API could further enhance its practical utility. This project not only provides a foundation for predictive credit scoring but also underscores the broader potential of machine learning in financial services. By enabling more accurate and equitable credit evaluations, it paves the way for smarter lending practices and improved customer experiences."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}